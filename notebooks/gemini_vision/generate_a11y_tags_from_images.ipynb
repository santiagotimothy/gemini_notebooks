{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1803c39-71f2-4564-bd69-3df66a1cb947",
   "metadata": {},
   "source": [
    "# Generate Accessibility tags from images using Gemini Vision\n",
    "The goal of this notebook is to generate [ARIA-label](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Attributes/aria-label) or mobile app accessibility label values from images.\n",
    "\n",
    "## Setup and requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966effb",
   "metadata": {},
   "source": [
    "### Install Google Cloud - Vertex AI libraries and [pillow](https://pypi.org/project/pillow/) imaging libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e959e-a819-4af5-8ad2-db5cf77f969e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required python packages and other dependencies\n",
    "!pip3 install --upgrade google-cloud-aiplatform # Library for using Multimodal AI models like Gemini/Gemini Vision/etc as a Google Cloud service \n",
    "\n",
    "!pip3 install --upgrade pillow # Image processing library (bitmap manipulation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13d4ac",
   "metadata": {},
   "source": [
    "### (When Necessary) Use the code below to authenticate your remote or local environment with Google Cloud\n",
    "__Note__: Skip cell if running on GCP remote Jupyterlab notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b92c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e8b96-1d74-4fee-903d-66e10ec82d46",
   "metadata": {},
   "source": [
    "### Restart current Python runtime\n",
    "\n",
    "- When new packages are installed/updated, restart the current Python runtime to apply the changes.\n",
    "- You can do this on VSCode by searching __Jupyter: Restart Kernel__ on the command palette or if you're using Jupyter Notebooks, clicking the Restart Kernel button in the top right of the notebooks IDE should do the trick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e078e2-2891-42ed-95f7-ce31795045d0",
   "metadata": {},
   "source": [
    "### Define Google Cloud project information and Initialize the Vertex AI SDK for Python for your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ca78d-1320-49dd-8c43-3d5608f47c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## IMPORTANT: Replace the variables below with the correct Project ID and location information for your authenticated account.\n",
    "import vertexai\n",
    "\n",
    "# Define project information and location\n",
    "PROJECT_ID = \"[YOUR_PROJECT_ID_HERE]\"\n",
    "LOCATION = \"australia-southeast1\"\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}, running on {LOCATION}\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb712712-c7fb-43ef-be19-2497d610111c",
   "metadata": {},
   "source": [
    "#### Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d32222-695d-4993-bdc4-fde7bc7948f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fad59-d08b-4aef-8230-7f567643ce14",
   "metadata": {},
   "source": [
    "#### Load Gemini 1.0 Pro Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd63f204-6acc-41aa-ac25-45544d5c3888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3a051-d567-4146-b428-fcb285c6485d",
   "metadata": {},
   "source": [
    "#### Define helper functions\n",
    "\n",
    "_taken from [GCP Gemini - Getting Started](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_python.ipynb)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de36ca-ded6-46e4-99e9-4ed2fc88ee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        print(content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68bb17-db90-437a-89b1-d1fe1d2e715f",
   "metadata": {},
   "source": [
    "### Image analysis\n",
    "\n",
    "#### Prepare images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41451360",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://foodhub.scene7.com/is/image/woolworthsltdprod/wk42-2024-carousel-img-cartology-pet-food?fmt=png-alpha&wid=1200&resMode=sharp2\"\n",
    "image = load_image_from_url(image_url)\n",
    "\n",
    "image_url2 = \"https://assets.woolworths.com.au/images/2010/283277.jpg?impolicy=wowcdxwbjbx&w=900&h=900\"\n",
    "image2 = load_image_from_url(image_url2)\n",
    "\n",
    "display_content_as_image(image)\n",
    "display_content_as_image(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682b239",
   "metadata": {},
   "source": [
    "#### Prompt Engineering for image analysis requests\n",
    "\n",
    "In multimodal requests that combine text and image prompts, prompt engineering plays a crucial role in ensuring Gemini interprets and integrates information from both sources effectively. Here's how it can be beneficial:\n",
    "\n",
    "* **Specifying Image Region of Interest:**  Text prompts can specify which part of the image to analyze. Imagine an image with multiple objects. A prompt like \"Analyze the text on the red box in the image\" would instruct Gemini to use the textual information alongside image recognition to decipher the text within the red box. \n",
    "\n",
    "When analysing a certain set of images with not enough of an idea of how it can be styled or formatted, it's enough to provide the model with a context of what to look for, like product images that contain a single object of interest, marketing images that might tell a story, etc.\n",
    "\n",
    "Text prompts can influence how Gemini interprets the image based on the provided context. For instance, an image of a person smiling might be interpreted differently with prompts like \"Analyze the facial expression of a doctor congratulating a patient\" compared to \"Analyze the facial expression of someone surprised by a birthday party.\"\n",
    "\n",
    "* **Instructing Desired Textual Output:** Text prompts can guide the type of textual response Gemini generates based on the image analysis. Prompts like \"Write a short story inspired by the image\" or \"Generate ONLY the GraphQL __JSON__ object containing the answers as __keys__ and the answers as __values__.\n",
    "\n",
    "* **Incorporate external context to the request:** Text prompts can incorporate additional information beyond what's visible in the image. Imagine if your image is a part of a marketing campaign or is a discounted item, where these indicators might be overlaid or placed alongside the image as critical indicators for users and you want those to be added to the image description, adding those as an additional context as a part of a [few-shot prompt](https://www.promptingguide.ai/techniques/fewshot) can help the request arrive with the desired output.\n",
    "\n",
    "By strategically using text prompts alongside image prompts, we can create a more nuanced and informative multimodal request for Gemini. This allows for a richer interaction with the model, leading to more accurate interpretations and desired text outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcae938-8299-4641-87d9-d78c3dc88ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"You are an expert content designer providing accessibility labels for images, abiding to WCAG 2.0 guidelines.\"\n",
    "instructions = \"Instructions: Consider the following images:\"\n",
    "query = \"\"\"Answer the following questions:\n",
    "ariaLabel: What is the appropriate ARIA-LABEL for the image for users with visual disabilities? Be as objective and descriptive as you can, describing how the item in the image is visually percieved, include all of the observable text and stickers in the image\n",
    "seoTags: What metadata tags will improve the searchability of this image and the website associated with it?\n",
    "\"\"\"\n",
    "format = \"Provide the output for each image as GraphQL-formatted JSON, each answer should be the value of each answer title as the JSON key, each answer should belong to a parent object named image[x], where x is the index of the image analyzed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca70a7",
   "metadata": {},
   "source": [
    "### Specify Vertex-specific hyperparameters\n",
    "\n",
    "Content Generation can be controlled to some extent by parameters such as __temperature__, __top-k__ and __top-p__. For this notebook, we will only be using temperature to control the output.\n",
    "\n",
    "__Temperature__: This is a hyperparameter that controls the randomness of the model's output, specifically when dealing with tasks like text generation or sampling sequences. It allows you to control the creativity and diversity of the model's output by influencing its exploration of the probability distribution during the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67053000",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "  \"temperature\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867dc55c",
   "metadata": {},
   "source": [
    "#### Wrap all of the input in a content dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390c4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = [\n",
    "    context,\n",
    "    instructions,\n",
    "    query,\n",
    "    format,\n",
    "    image,\n",
    "    image2\n",
    "]\n",
    "\n",
    "print_multimodal_prompt(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4aad87",
   "metadata": {},
   "source": [
    "#### Generate responses from the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156c265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = multimodal_model.generate_content(contents, generation_config=generation_config, stream=False)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28025b0",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92127e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(responses.text)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-15.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-15:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
